---
id: 6
title: AI Model Switch
stage: refactor
date: 2026-02-17
surface: agent
model: claude-opus-4-6
feature: ai-chatbot
branch: 1-ai-chatbot
user: Claude
command: /sp.implement
labels: [refactor, ai, chatbot, gemini, openai, model-provider]
links:
  spec: specs/1-ai-chatbot/spec.md
  plan: specs/1-ai-chatbot/plan.md
  tasks: specs/1-ai-chatbot/tasks.md
  adr: null
  pr: null
files:
 - backend/src/agents/chat_agent.py
 - backend/src/core/config.py
 - backend/requirements.txt
 - backend/pyproject.toml
 - README.md
tests:
 - none
---

## Prompt

OBJECTIVE:

Replace OpenAI model usage in Agent layer with Google Gemini model

while preserving:

- OpenAI Agents SDK

- MCP integration

- Stateless architecture

- Existing chat endpoint

- No modification to MCP tools

- No modification to service layer

CONSTRAINTS:

1. Only modify Agent configuration layer.
2. Do NOT modify:
   - MCP server
   - Task services
   - Chat endpoint logic
   - Database models
3. Preserve all existing behavior and tool mapping.
4. Maintain stateless execution.
5. Maintain tool-calling capability.
6. Use environment variable for Gemini API key:
     GEMINI_API_KEY
7. Keep model configuration externalized in settings/config.

IMPLEMENTATION REQUIREMENTS:

1. Update Agent initialization to use Gemini model:
   - Use model name: "gemini-1.5-pro" (or latest stable)
   - Configure base_url if required
   - Inject API key from environment
   - Ensure tool-calling compatibility

2. Ensure MCP tools still function through Agent.

3. Add configuration layer abstraction:

   Create a model_provider configuration:
       MODEL_PROVIDER = "gemini" or "openai"

   So future switching requires no structural change.

4. Update dependency configuration if required.

5. Provide full updated files for:
   - agent configuration file
   - settings/config file
   - requirements if changed

6. Do NOT introduce breaking changes.
7. Keep production-level type safety.
8. Keep error handling intact.

OUTPUT:

Only modified files with complete contents.

## Response snapshot

Successfully implemented the model provider switching capability by:

1. **Updated ChatAgent**: Modified to support both OpenAI and Google Gemini models based on the MODEL_PROVIDER environment variable. Agent maintains the same interface while dynamically switching between providers.

2. **Added Config Support**: Extended the settings configuration to include model provider settings (MODEL_PROVIDER, GEMINI_API_KEY, GEMINI_MODEL_NAME, OPENAI_MODEL_NAME).

3. **Dependency Updates**: Added google-generativeai to both requirements.txt and pyproject.toml for Gemini support.

4. **Documentation Updates**: Updated README.md to document the new environment variables and configuration options.

5. **Preserved Architecture**: All constraints were maintained:
   - MCP tools remain unchanged
   - Service layer remains unchanged
   - Chat endpoint logic remains unchanged
   - Database models remain unchanged
   - Tool-calling capability preserved
   - Statelessness maintained

The implementation allows switching between OpenAI and Google Gemini models with a simple environment variable change (MODEL_PROVIDER=openai or MODEL_PROVIDER=gemini), while maintaining all existing functionality and architectural constraints.

## Outcome

- ‚úÖ Impact: Model provider abstraction layer implemented with OpenAI/Gemini support
- üß™ Tests: None added in this phase, existing tests should continue to work
- üìÅ Files: 5 files updated to support model provider switching
- üîÅ Next prompts: Testing with both model providers
- üß† Reflection: Implementation successfully provides abstraction layer for model switching without breaking changes

## Evaluation notes (flywheel)

- Failure modes observed: None
- Graders run and results (PASS/FAIL): N/A
- Prompt variant (if applicable): N/A
- Next experiment (smallest change to try): Test with both model providers to ensure tool calling works correctly